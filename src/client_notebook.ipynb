{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Set, List, Union, Tuple, Any\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.gptdataset import create_dataloader_v1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_data:str = \"\"\"To Sherlock Holmes she is always the woman. I have seldom heard him\n",
    "     mention her under any other name. In his eyes she eclipses and\n",
    "     predominates the whole of her sex. It was not that he felt any\n",
    "     emotion akin to love for Irene Adler. All emotions, and that one\n",
    "     particularly, were abhorrent to his cold, precise but admirably\n",
    "     balanced mind. He was, I take it, the most perfect reasoning and\n",
    "     observing machine that the world has seen, but as a lover he would\n",
    "     have placed himself in a false position. He never spoke of the softer\n",
    "     passions, save with a gibe and a sneer. They were admirable things\n",
    "     for the observer--excellent for drawing the veil from men's motives\n",
    "     and actions. But for the trained reasoner to admit such intrusions\n",
    "     into his own delicate and finely adjusted temperament was to\n",
    "     introduce a distracting factor which might throw a doubt upon all his\n",
    "     mental results. Grit in a sensitive instrument, or a crack in one of\n",
    "     his own high-power lenses, would not be more disturbing than a strong\n",
    "     emotion in a nature such as his. And yet there was but one woman to\n",
    "     him, and that woman was the late Irene Adler, of dubious and\n",
    "     questionable memory.\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = create_dataloader_v1(txt_data, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2514, 25730, 17628,   673,   318,  1464,   262,  2415,    13,   314,\n",
      "           423, 25129,  2982,   683,   198,   220,   220,   220,   220,  3068,\n",
      "           607,   739,   597,   584,  1438,    13,   554,   465,  2951,   673,\n",
      "         39097,   274,   290,   198,   220,   220,   220,   220, 15960,   689,\n",
      "           262,  2187,   286,   607,  1714,    13,   632,   373,   407,   326,\n",
      "           339,  2936,   597,   198,   220,   220,   220,   220,  9942, 22107,\n",
      "           284,  1842,   329,  7181,   710,  1215,  1754,    13,  1439, 10825,\n",
      "            11,   290,   326,   530,   198,   220,   220,   220,   220,  3573,\n",
      "            11,   547, 42934,  1156,   284,   465,  4692,    11,  7141,   475,\n",
      "          6178,   343,  1346,   198,   220,   220,   220,   220, 12974,  2000,\n",
      "            13,   679,   373,    11,   314,  1011,   340,    11,   262,   749,\n",
      "          2818, 14607,   290,   198,   220,   220,   220,   220, 21769,  4572,\n",
      "           326,   262,   995,   468,  1775,    11,   475,   355,   257, 18854,\n",
      "           339,   561,   198,   220,   220,   220,   220,   423,  4624,  2241,\n",
      "           287,   257,  3991,  2292,    13,   679,  1239,  5158,   286,   262,\n",
      "         32359,   198,   220,   220,   220,   220, 30477,    11,  3613,   351,\n",
      "           257,   308, 32438,   290,   257, 10505,   263,    13,  1119,   547,\n",
      "         37959,  1243,   198,   220,   220,   220,   220,   329,   262, 22890,\n",
      "           438,  1069,  5666,   329,  8263,   262, 30615,   422,  1450,   338,\n",
      "         21508,   198,   220,   220,   220,   220,   290,  4028,    13,   887,\n",
      "           329,   262,  8776,  1738,   263,   284,  9159,   884,  9913, 15880,\n",
      "           198,   220,   220,   220,   220,   656,   465,   898, 19217,   290,\n",
      "         32566, 12328, 36140,   373,   284,   198,   220,   220,   220,   220,\n",
      "         10400,   257, 36441,  5766,   543,  1244,  3714,   257,  4719,  2402,\n",
      "           477,   465,   198,   220,   220,   220,   220,  5110,  2482,    13,\n",
      "           402,   799,   287,   257,  8564,  8875]])\n",
      "tensor([[25730, 17628,   673,   318,  1464,   262,  2415,    13,   314,   423,\n",
      "         25129,  2982,   683,   198,   220,   220,   220,   220,  3068,   607,\n",
      "           739,   597,   584,  1438,    13,   554,   465,  2951,   673, 39097,\n",
      "           274,   290,   198,   220,   220,   220,   220, 15960,   689,   262,\n",
      "          2187,   286,   607,  1714,    13,   632,   373,   407,   326,   339,\n",
      "          2936,   597,   198,   220,   220,   220,   220,  9942, 22107,   284,\n",
      "          1842,   329,  7181,   710,  1215,  1754,    13,  1439, 10825,    11,\n",
      "           290,   326,   530,   198,   220,   220,   220,   220,  3573,    11,\n",
      "           547, 42934,  1156,   284,   465,  4692,    11,  7141,   475,  6178,\n",
      "           343,  1346,   198,   220,   220,   220,   220, 12974,  2000,    13,\n",
      "           679,   373,    11,   314,  1011,   340,    11,   262,   749,  2818,\n",
      "         14607,   290,   198,   220,   220,   220,   220, 21769,  4572,   326,\n",
      "           262,   995,   468,  1775,    11,   475,   355,   257, 18854,   339,\n",
      "           561,   198,   220,   220,   220,   220,   423,  4624,  2241,   287,\n",
      "           257,  3991,  2292,    13,   679,  1239,  5158,   286,   262, 32359,\n",
      "           198,   220,   220,   220,   220, 30477,    11,  3613,   351,   257,\n",
      "           308, 32438,   290,   257, 10505,   263,    13,  1119,   547, 37959,\n",
      "          1243,   198,   220,   220,   220,   220,   329,   262, 22890,   438,\n",
      "          1069,  5666,   329,  8263,   262, 30615,   422,  1450,   338, 21508,\n",
      "           198,   220,   220,   220,   220,   290,  4028,    13,   887,   329,\n",
      "           262,  8776,  1738,   263,   284,  9159,   884,  9913, 15880,   198,\n",
      "           220,   220,   220,   220,   656,   465,   898, 19217,   290, 32566,\n",
      "         12328, 36140,   373,   284,   198,   220,   220,   220,   220, 10400,\n",
      "           257, 36441,  5766,   543,  1244,  3714,   257,  4719,  2402,   477,\n",
      "           465,   198,   220,   220,   220,   220,  5110,  2482,    13,   402,\n",
      "           799,   287,   257,  8564,  8875,    11]])\n"
     ]
    }
   ],
   "source": [
    "for i, j in dl:\n",
    "    print(i)\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suleyman/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/suleyman/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "hf_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2514, 25730, 17628, 673, 318]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer(txt_data)['input_ids'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
